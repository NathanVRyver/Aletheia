base_model: deepseek-ai/deepseek-coder-7b-instruct-v1.5
dataset_path: ./data/sft/*.jsonl
output_dir: ./checkpoints/aletheia-sft
max_seq_len: 4096
packing: true
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
epochs: 2
lr: 2e-4
lr_scheduler: cosine
warmup_ratio: 0.03
weight_decay: 0.0
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
precision:
  load_8bit: true
logging_steps: 20
save_steps: 1000