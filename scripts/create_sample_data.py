#!/usr/bin/env python3
"""
Create sample training data for Aletheia project
Focus: Systems/backend engineering and reasoning tasks
"""
import json
import os
from typing import List, Dict

def create_sft_samples() -> List[Dict]:
    """Create SFT training samples focused on systems/backend engineering"""
    samples = [
        {
            "prompt": "Explain the differences between epoll and kqueue for high-performance network servers.",
            "response": "epoll (Linux) and kqueue (BSD/macOS) are both scalable I/O event notification mechanisms, but differ in design:\n\n**epoll:**\n- Edge-triggered and level-triggered modes\n- Uses file descriptors: epoll_create(), epoll_ctl(), epoll_wait()\n- Better for large numbers of mostly-idle connections\n- O(1) performance for add/remove/modify operations\n\n**kqueue:**\n- More general event system (not just I/O)\n- Uses kevent() system call\n- Can monitor files, processes, signals, timers\n- Slightly more complex API but more flexible\n\n**Trade-offs:**\n- epoll: Linux-only, simpler for network I/O\n- kqueue: More portable across BSDs, more feature-rich\n- Both outperform select/poll for >1000 concurrent connections\n\nFor cross-platform code, use libraries like libuv or libev that abstract these differences."
        },
        {
            "prompt": "Design a Redis cluster setup for handling high write loads with failover capabilities.",
            "response": "Here's a robust Redis cluster design for high write loads:\n\n**Architecture:**\n1. **Redis Cluster Mode**: 6+ nodes (3 masters, 3 replicas minimum)\n2. **Sharding**: Data automatically partitioned across masters using hash slots (16384 total)\n3. **Replication**: Each master has 1+ replicas for failover\n\n**Configuration:**\n```\n# redis.conf key settings\ncluster-enabled yes\ncluster-config-file nodes.conf\ncluster-node-timeout 5000\nappendonly yes\nappendfsync everysec\n```\n\n**Write Optimization:**\n- Use pipelining for batch operations\n- Configure appropriate `maxmemory-policy` (allkeys-lru)\n- Tune `tcp-keepalive` and `timeout` settings\n- Consider Redis 6.0+ threaded I/O\n\n**Failover:**\n- Automatic failover when majority nodes detect master failure\n- Manual failover: `CLUSTER FAILOVER`\n- Monitor with Redis Sentinel for additional reliability\n\n**Monitoring:**\n- Track `instantaneous_ops_per_sec`, `used_memory`, `keyspace_misses`\n- Set up alerts for node failures and high latency\n\nThis setup handles node failures gracefully while maintaining write performance."
        },
        {
            "prompt": "What are the key considerations when implementing a distributed rate limiter?",
            "response": "Implementing a distributed rate limiter requires careful consideration of several factors:\n\n**Storage Backend:**\n- **Redis**: Fast, supports atomic operations (INCR, EXPIRE), good for sliding window\n- **Database**: More durable but slower, suitable for longer-term limits\n- **In-memory + sync**: Fastest but requires coordination\n\n**Algorithm Choice:**\n1. **Token Bucket**: Allows bursts, smooth rate limiting\n2. **Fixed Window**: Simple but has boundary issues\n3. **Sliding Window**: More accurate but memory-intensive\n4. **Sliding Window Log**: Most accurate, highest memory usage\n\n**Key Design Decisions:**\n- **Partitioning**: By user ID, IP, API key - affects accuracy vs performance\n- **Consistency**: Strong consistency (slower) vs eventual consistency (faster)\n- **Fallback**: Fail open (allow requests) vs fail closed (deny requests)\n\n**Implementation Considerations:**\n```python\n# Example Redis Lua script for atomic operations\nlocal key = KEYS[1]\nlocal limit = tonumber(ARGV[1])\nlocal window = tonumber(ARGV[2])\nlocal current = redis.call('INCR', key)\nif current == 1 then\n    redis.call('EXPIRE', key, window)\nend\nreturn {current, limit}\n```\n\n**Performance:**\n- Cache frequently accessed limits locally\n- Use batching for high-traffic scenarios\n- Consider async updates for better latency\n\n**Monitoring:**\n- Track hit rates, false positives/negatives\n- Monitor backend latency and availability\n- Set up alerts for rate limiter failures"
        },
        {
            "prompt": "How would you debug a memory leak in a Go application?",
            "response": "Debugging Go memory leaks requires systematic analysis using Go's built-in tools:\n\n**1. Enable pprof in your application:**\n```go\nimport _ \"net/http/pprof\"\ngo func() {\n    log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n}()\n```\n\n**2. Collect heap profiles:**\n```bash\n# Take snapshots over time\ngo tool pprof http://localhost:6060/debug/pprof/heap\ngo tool pprof -base heap1.pb.gz heap2.pb.gz  # Compare profiles\n```\n\n**3. Analyze with pprof commands:**\n```\n(pprof) top10          # Top memory consumers\n(pprof) list funcname  # Source code with annotations\n(pprof) web           # Visual graph\n(pprof) traces        # Allocation traces\n```\n\n**4. Common leak patterns to look for:**\n- **Goroutine leaks**: `go tool pprof /debug/pprof/goroutine`\n- **Unclosed resources**: files, network connections, channels\n- **Growing slices/maps**: Check for unbounded growth\n- **Event listeners**: Not properly removed\n- **Caches**: Without TTL or size limits\n\n**5. Memory debugging flags:**\n```bash\nGOMEMLIMIT=1GiB        # Set soft memory limit\nGOGC=50               # More aggressive GC\nGODEBUG=gctrace=1     # GC trace output\n```\n\n**6. Prevention strategies:**\n- Use `defer` for cleanup\n- Implement context cancellation\n- Set timeouts on network operations\n- Use sync.Pool for object reuse\n- Regular profile reviews in CI\n\nAlways reproduce the leak in a controlled environment and take multiple heap dumps to identify growth patterns."
        },
        {
            "prompt": "Explain the CAP theorem and its practical implications for database design.",
            "response": "The CAP theorem states that in a distributed system, you can only guarantee 2 out of 3 properties:\n\n**Consistency (C)**: All nodes see the same data simultaneously\n**Availability (A)**: System remains operational even during failures\n**Partition tolerance (P)**: System continues despite network partitions\n\n**Practical Implications:**\n\nSince network partitions are inevitable in distributed systems, you must choose between CP or AP:\n\n**CP Systems (Consistency + Partition tolerance):**\n- Examples: MongoDB (default), Redis Cluster, Consul\n- Behavior: Become unavailable during partitions to maintain consistency\n- Use cases: Financial transactions, inventory management\n- Trade-off: May reject writes/reads during network issues\n\n**AP Systems (Availability + Partition tolerance):**\n- Examples: Cassandra, DynamoDB, CouchDB\n- Behavior: Remain available but may serve stale data\n- Use cases: Social media feeds, analytics, caching\n- Trade-off: Eventual consistency, potential data conflicts\n\n**Real-world Considerations:**\n\n1. **Tunable Consistency**: Many systems offer configurable consistency levels\n   - Cassandra: ONE, QUORUM, ALL\n   - MongoDB: Read/write concerns\n\n2. **PACELC Extension**: In the absence of partitions, trade-off between Latency and Consistency\n\n3. **Design Patterns:**\n   - **CQRS**: Separate read/write models\n   - **Event sourcing**: Append-only event logs\n   - **Saga pattern**: Distributed transactions via compensating actions\n\n**Choosing Your Trade-off:**\n- Financial systems: CP (strong consistency required)\n- Social platforms: AP (availability more important than perfect consistency)\n- E-commerce: Hybrid approach (inventory=CP, recommendations=AP)\n\nThe key is understanding your specific consistency requirements and choosing the appropriate system and configuration."
        },
        {
            "prompt": "What are the best practices for implementing circuit breakers in microservices?",
            "response": "Circuit breakers prevent cascading failures by monitoring service health and failing fast when dependencies are unhealthy. Here are key implementation practices:\n\n**Core States:**\n1. **CLOSED**: Normal operation, requests pass through\n2. **OPEN**: Requests fail immediately, service is considered down\n3. **HALF-OPEN**: Testing if service has recovered\n\n**Configuration Parameters:**\n```yaml\ncircuit_breaker:\n  failure_threshold: 5        # Failures to trigger OPEN\n  success_threshold: 2        # Successes to close from HALF-OPEN\n  timeout: 60s               # Time before trying HALF-OPEN\n  monitoring_window: 10s     # Window for failure rate calculation\n  failure_rate_threshold: 50  # Percentage threshold\n```\n\n**Implementation Best Practices:**\n\n1. **Granular Circuit Breakers:**\n   - Per-service, per-endpoint, or per-operation\n   - Different thresholds for critical vs non-critical operations\n\n2. **Failure Detection:**\n```go\n// Example criteria for failures\ntype FailureDetector interface {\n    IsFailure(err error, response *http.Response) bool\n}\n\n// Consider failures: timeouts, 5xx errors, connection errors\n// NOT failures: 4xx client errors, business logic errors\n```\n\n3. **Fallback Strategies:**\n   - **Cache**: Return stale data\n   - **Default values**: Safe defaults or empty responses\n   - **Alternative service**: Secondary data source\n   - **Degraded functionality**: Partial feature set\n\n4. **Monitoring and Alerting:**\n```yaml\nmetrics:\n  - circuit_breaker_state (gauge)\n  - circuit_breaker_calls_total (counter)\n  - circuit_breaker_failures_total (counter)\n  - circuit_breaker_fallback_calls_total (counter)\n```\n\n5. **Testing:**\n   - Chaos engineering to test circuit breaker behavior\n   - Load testing with dependency failures\n   - Verify fallback mechanisms work correctly\n\n6. **Bulkhead Pattern Integration:**\n   - Separate thread pools for different dependencies\n   - Isolate failures to prevent resource exhaustion\n\n**Libraries:**\n- **Go**: sony/gobreaker, rubyist/circuitbreaker\n- **Java**: Hystrix, Resilience4j\n- **Python**: pybreaker, circuit-breaker\n\n**Avoid Common Pitfalls:**\n- Don't make circuit breaker logic too complex\n- Ensure fallbacks are also resilient\n- Monitor and tune thresholds based on real traffic patterns\n- Consider the impact of circuit breakers on user experience"
        }
    ]
    return samples

def create_dpo_samples() -> List[Dict]:
    """Create DPO pairs with chosen/rejected responses"""
    samples = [
        {
            "prompt": "How do you handle database connection pooling in a high-traffic web application?",
            "chosen": "Database connection pooling is crucial for high-traffic applications. Here's a comprehensive approach:\n\n**Connection Pool Configuration:**\n```python\n# Example with SQLAlchemy\nengine = create_engine(\n    'postgresql://user:pass@host/db',\n    pool_size=20,          # Core connections\n    max_overflow=30,       # Additional connections\n    pool_timeout=30,       # Wait time for connection\n    pool_recycle=3600,     # Recycle connections hourly\n    pool_pre_ping=True     # Validate connections\n)\n```\n\n**Best Practices:**\n1. **Sizing**: Start with pool_size = 2 * CPU cores, tune based on monitoring\n2. **Connection Lifecycle**: Set appropriate timeouts and recycling\n3. **Health Checks**: Use pool_pre_ping to avoid stale connections\n4. **Monitoring**: Track active/idle connections, wait times, pool exhaustion\n\n**High-Traffic Optimizations:**\n- Use read replicas for read-only queries\n- Implement connection sharding for massive scale\n- Consider PgBouncer/connection proxy for additional pooling layer\n- Cache frequently accessed data to reduce DB load\n\n**Error Handling:**\n- Implement circuit breakers for DB failures\n- Have fallback strategies for pool exhaustion\n- Log and alert on connection pool metrics\n\nProper pooling can improve performance 10-100x by eliminating connection overhead.",
            "rejected": "Just use a database connection pool library like HikariCP or SQLAlchemy's pool. Set the pool size to something reasonable like 10-20 connections and you should be good. Make sure to close connections when you're done with them. You might also want to use connection timeouts to avoid hanging connections."
        },
        {
            "prompt": "What's the difference between horizontal and vertical scaling?",
            "chosen": "Horizontal and vertical scaling are two fundamental approaches to handling increased system load:\n\n**Vertical Scaling (Scale Up):**\n- Add more power to existing machines (CPU, RAM, storage)\n- Pros: Simple to implement, no code changes needed, strong consistency\n- Cons: Hardware limits, single point of failure, expensive at high end\n- Example: Upgrading from 8GB to 32GB RAM\n\n**Horizontal Scaling (Scale Out):**\n- Add more machines to the pool of resources\n- Pros: Virtually unlimited scaling, better fault tolerance, cost-effective\n- Cons: Complex architecture, eventual consistency challenges, network overhead\n- Example: Adding 5 more web servers behind a load balancer\n\n**When to Choose Each:**\n\n**Vertical Scaling:**\n- Databases requiring ACID properties (traditional RDBMS)\n- Applications with shared state that's hard to partition\n- Legacy systems not designed for distribution\n- Development/testing environments\n\n**Horizontal Scaling:**\n- Web applications with stateless services\n- Big data processing (Hadoop, Spark)\n- NoSQL databases (Cassandra, MongoDB)\n- Microservices architectures\n\n**Hybrid Approach:**\nMost modern systems use both:\n- Vertical scaling for databases up to a point\n- Horizontal scaling for application tiers\n- Auto-scaling groups that add instances (horizontal) with appropriate instance sizes (vertical)\n\n**Real-world Example:**\nNetflix uses horizontal scaling extensively - thousands of microservices across multiple AWS regions, with each service scaled independently based on demand patterns.",
            "rejected": "Vertical scaling means making your servers bigger and more powerful. Horizontal scaling means adding more servers. Vertical is easier but has limits, horizontal is more complex but can scale infinitely. Most companies use horizontal scaling for web apps and vertical scaling for databases."
        },
        {
            "prompt": "How would you implement caching in a distributed system?",
            "chosen": "Implementing caching in distributed systems requires careful consideration of consistency, performance, and complexity trade-offs:\n\n**Cache Hierarchy:**\n1. **Browser Cache**: Static assets, API responses with cache headers\n2. **CDN/Edge Cache**: Geographic distribution, static content\n3. **Application Cache**: In-memory (Redis, Memcached), application-specific\n4. **Database Cache**: Query result caching, buffer pools\n\n**Distributed Cache Strategies:**\n\n**1. Cache-Aside (Lazy Loading):**\n```python\ndef get_user(user_id):\n    # Try cache first\n    user = cache.get(f\"user:{user_id}\")\n    if user is None:\n        # Cache miss - fetch from DB\n        user = db.get_user(user_id)\n        cache.set(f\"user:{user_id}\", user, ttl=3600)\n    return user\n```\n\n**2. Write-Through:**\n- Write to cache and database simultaneously\n- Ensures cache consistency but adds latency\n\n**3. Write-Behind (Write-Back):**\n- Write to cache immediately, database asynchronously\n- Better performance but risk of data loss\n\n**Consistency Patterns:**\n\n**Cache Invalidation:**\n```python\n# Event-driven invalidation\ndef update_user(user_id, data):\n    db.update_user(user_id, data)\n    cache.delete(f\"user:{user_id}\")\n    # Publish cache invalidation event\n    publish_event('user_updated', user_id)\n```\n\n**Time-Based Expiration:**\n- TTL for different data types based on change frequency\n- Jittered expiration to avoid cache stampedes\n\n**Version-Based Invalidation:**\n- Include version numbers in cache keys\n- Increment version on updates\n\n**Distributed Cache Considerations:**\n\n**1. Partitioning:**\n- Consistent hashing for even distribution\n- Replication for high availability\n\n**2. Cache Stampede Prevention:**\n```python\nimport threading\nfrom contextlib import contextmanager\n\n@contextmanager\ndef cache_lock(key):\n    lock = threading.Lock()\n    try:\n        if lock.acquire(timeout=5):\n            yield\n        else:\n            # Return stale data or default\n            pass\n    finally:\n        lock.release()\n```\n\n**3. Multi-Level Caching:**\n```\nUser Request → L1 (Local Memory) → L2 (Redis Cluster) → Database\n```\n\n**Monitoring:**\n- Hit/miss ratios by cache level\n- Cache latency and throughput\n- Memory usage and eviction rates\n- Cache consistency violations\n\n**Technology Choices:**\n- **Redis**: Rich data structures, clustering, persistence\n- **Memcached**: Simple, high performance, multi-threaded\n- **Caffeine**: High-performance local cache (Java)\n- **Application-level**: Custom caching with specific eviction policies\n\n**Best Practices:**\n- Cache what's expensive to compute/fetch\n- Use appropriate TTLs based on data change patterns\n- Implement graceful degradation when cache is unavailable\n- Monitor and tune cache performance continuously\n- Design for cache failures - never assume cache is always available",
            "rejected": "You can implement caching using Redis or Memcached. Put frequently accessed data in the cache with a TTL (time to live). Use cache-aside pattern where you check the cache first, and if it's a miss, fetch from the database and put it in cache. Make sure to invalidate the cache when data changes. You can also use multiple cache layers like local cache + distributed cache for better performance."
        }
    ]
    return samples

def save_jsonl(data: List[Dict], filepath: str):
    """Save data as JSONL format"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w') as f:
        for item in data:
            f.write(json.dumps(item) + '\n')

def main():
    """Create sample training data"""
    print("Creating sample training data...")
    
    # Create SFT data
    sft_samples = create_sft_samples()
    save_jsonl(sft_samples, 'data/sft/systems_engineering.jsonl')
    print(f"Created {len(sft_samples)} SFT samples")
    
    # Create DPO data
    dpo_samples = create_dpo_samples()
    save_jsonl(dpo_samples, 'data/dpo/preference_pairs.jsonl')
    print(f"Created {len(dpo_samples)} DPO pairs")
    
    # Create eval data (subset of SFT for testing)
    eval_samples = sft_samples[:2]  # Use first 2 for eval
    save_jsonl(eval_samples, 'data/eval/systems_eval.jsonl')
    print(f"Created {len(eval_samples)} evaluation samples")
    
    print("Sample data creation complete!")
    print("\nNext steps:")
    print("1. Run: source .venv/bin/activate")
    print("2. Run: pip install -r requirements.txt") 
    print("3. Run: make sft (to start training)")

if __name__ == "__main__":
    main()
