{"prompt": "How do you handle database connection pooling in a high-traffic web application?", "chosen": "Database connection pooling is crucial for high-traffic applications. Here's a comprehensive approach:\n\n**Connection Pool Configuration:**\n```python\n# Example with SQLAlchemy\nengine = create_engine(\n    'postgresql://user:pass@host/db',\n    pool_size=20,          # Core connections\n    max_overflow=30,       # Additional connections\n    pool_timeout=30,       # Wait time for connection\n    pool_recycle=3600,     # Recycle connections hourly\n    pool_pre_ping=True     # Validate connections\n)\n```\n\n**Best Practices:**\n1. **Sizing**: Start with pool_size = 2 * CPU cores, tune based on monitoring\n2. **Connection Lifecycle**: Set appropriate timeouts and recycling\n3. **Health Checks**: Use pool_pre_ping to avoid stale connections\n4. **Monitoring**: Track active/idle connections, wait times, pool exhaustion\n\n**High-Traffic Optimizations:**\n- Use read replicas for read-only queries\n- Implement connection sharding for massive scale\n- Consider PgBouncer/connection proxy for additional pooling layer\n- Cache frequently accessed data to reduce DB load\n\n**Error Handling:**\n- Implement circuit breakers for DB failures\n- Have fallback strategies for pool exhaustion\n- Log and alert on connection pool metrics\n\nProper pooling can improve performance 10-100x by eliminating connection overhead.", "rejected": "Just use a database connection pool library like HikariCP or SQLAlchemy's pool. Set the pool size to something reasonable like 10-20 connections and you should be good. Make sure to close connections when you're done with them. You might also want to use connection timeouts to avoid hanging connections."}
{"prompt": "What's the difference between horizontal and vertical scaling?", "chosen": "Horizontal and vertical scaling are two fundamental approaches to handling increased system load:\n\n**Vertical Scaling (Scale Up):**\n- Add more power to existing machines (CPU, RAM, storage)\n- Pros: Simple to implement, no code changes needed, strong consistency\n- Cons: Hardware limits, single point of failure, expensive at high end\n- Example: Upgrading from 8GB to 32GB RAM\n\n**Horizontal Scaling (Scale Out):**\n- Add more machines to the pool of resources\n- Pros: Virtually unlimited scaling, better fault tolerance, cost-effective\n- Cons: Complex architecture, eventual consistency challenges, network overhead\n- Example: Adding 5 more web servers behind a load balancer\n\n**When to Choose Each:**\n\n**Vertical Scaling:**\n- Databases requiring ACID properties (traditional RDBMS)\n- Applications with shared state that's hard to partition\n- Legacy systems not designed for distribution\n- Development/testing environments\n\n**Horizontal Scaling:**\n- Web applications with stateless services\n- Big data processing (Hadoop, Spark)\n- NoSQL databases (Cassandra, MongoDB)\n- Microservices architectures\n\n**Hybrid Approach:**\nMost modern systems use both:\n- Vertical scaling for databases up to a point\n- Horizontal scaling for application tiers\n- Auto-scaling groups that add instances (horizontal) with appropriate instance sizes (vertical)\n\n**Real-world Example:**\nNetflix uses horizontal scaling extensively - thousands of microservices across multiple AWS regions, with each service scaled independently based on demand patterns.", "rejected": "Vertical scaling means making your servers bigger and more powerful. Horizontal scaling means adding more servers. Vertical is easier but has limits, horizontal is more complex but can scale infinitely. Most companies use horizontal scaling for web apps and vertical scaling for databases."}
{"prompt": "How would you implement caching in a distributed system?", "chosen": "Implementing caching in distributed systems requires careful consideration of consistency, performance, and complexity trade-offs:\n\n**Cache Hierarchy:**\n1. **Browser Cache**: Static assets, API responses with cache headers\n2. **CDN/Edge Cache**: Geographic distribution, static content\n3. **Application Cache**: In-memory (Redis, Memcached), application-specific\n4. **Database Cache**: Query result caching, buffer pools\n\n**Distributed Cache Strategies:**\n\n**1. Cache-Aside (Lazy Loading):**\n```python\ndef get_user(user_id):\n    # Try cache first\n    user = cache.get(f\"user:{user_id}\")\n    if user is None:\n        # Cache miss - fetch from DB\n        user = db.get_user(user_id)\n        cache.set(f\"user:{user_id}\", user, ttl=3600)\n    return user\n```\n\n**2. Write-Through:**\n- Write to cache and database simultaneously\n- Ensures cache consistency but adds latency\n\n**3. Write-Behind (Write-Back):**\n- Write to cache immediately, database asynchronously\n- Better performance but risk of data loss\n\n**Consistency Patterns:**\n\n**Cache Invalidation:**\n```python\n# Event-driven invalidation\ndef update_user(user_id, data):\n    db.update_user(user_id, data)\n    cache.delete(f\"user:{user_id}\")\n    # Publish cache invalidation event\n    publish_event('user_updated', user_id)\n```\n\n**Time-Based Expiration:**\n- TTL for different data types based on change frequency\n- Jittered expiration to avoid cache stampedes\n\n**Version-Based Invalidation:**\n- Include version numbers in cache keys\n- Increment version on updates\n\n**Distributed Cache Considerations:**\n\n**1. Partitioning:**\n- Consistent hashing for even distribution\n- Replication for high availability\n\n**2. Cache Stampede Prevention:**\n```python\nimport threading\nfrom contextlib import contextmanager\n\n@contextmanager\ndef cache_lock(key):\n    lock = threading.Lock()\n    try:\n        if lock.acquire(timeout=5):\n            yield\n        else:\n            # Return stale data or default\n            pass\n    finally:\n        lock.release()\n```\n\n**3. Multi-Level Caching:**\n```\nUser Request \u2192 L1 (Local Memory) \u2192 L2 (Redis Cluster) \u2192 Database\n```\n\n**Monitoring:**\n- Hit/miss ratios by cache level\n- Cache latency and throughput\n- Memory usage and eviction rates\n- Cache consistency violations\n\n**Technology Choices:**\n- **Redis**: Rich data structures, clustering, persistence\n- **Memcached**: Simple, high performance, multi-threaded\n- **Caffeine**: High-performance local cache (Java)\n- **Application-level**: Custom caching with specific eviction policies\n\n**Best Practices:**\n- Cache what's expensive to compute/fetch\n- Use appropriate TTLs based on data change patterns\n- Implement graceful degradation when cache is unavailable\n- Monitor and tune cache performance continuously\n- Design for cache failures - never assume cache is always available", "rejected": "You can implement caching using Redis or Memcached. Put frequently accessed data in the cache with a TTL (time to live). Use cache-aside pattern where you check the cache first, and if it's a miss, fetch from the database and put it in cache. Make sure to invalidate the cache when data changes. You can also use multiple cache layers like local cache + distributed cache for better performance."}
