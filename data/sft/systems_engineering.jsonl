{"prompt": "Explain the differences between epoll and kqueue for high-performance network servers.", "response": "epoll (Linux) and kqueue (BSD/macOS) are both scalable I/O event notification mechanisms, but differ in design:\n\n**epoll:**\n- Edge-triggered and level-triggered modes\n- Uses file descriptors: epoll_create(), epoll_ctl(), epoll_wait()\n- Better for large numbers of mostly-idle connections\n- O(1) performance for add/remove/modify operations\n\n**kqueue:**\n- More general event system (not just I/O)\n- Uses kevent() system call\n- Can monitor files, processes, signals, timers\n- Slightly more complex API but more flexible\n\n**Trade-offs:**\n- epoll: Linux-only, simpler for network I/O\n- kqueue: More portable across BSDs, more feature-rich\n- Both outperform select/poll for >1000 concurrent connections\n\nFor cross-platform code, use libraries like libuv or libev that abstract these differences."}
{"prompt": "Design a Redis cluster setup for handling high write loads with failover capabilities.", "response": "Here's a robust Redis cluster design for high write loads:\n\n**Architecture:**\n1. **Redis Cluster Mode**: 6+ nodes (3 masters, 3 replicas minimum)\n2. **Sharding**: Data automatically partitioned across masters using hash slots (16384 total)\n3. **Replication**: Each master has 1+ replicas for failover\n\n**Configuration:**\n```\n# redis.conf key settings\ncluster-enabled yes\ncluster-config-file nodes.conf\ncluster-node-timeout 5000\nappendonly yes\nappendfsync everysec\n```\n\n**Write Optimization:**\n- Use pipelining for batch operations\n- Configure appropriate `maxmemory-policy` (allkeys-lru)\n- Tune `tcp-keepalive` and `timeout` settings\n- Consider Redis 6.0+ threaded I/O\n\n**Failover:**\n- Automatic failover when majority nodes detect master failure\n- Manual failover: `CLUSTER FAILOVER`\n- Monitor with Redis Sentinel for additional reliability\n\n**Monitoring:**\n- Track `instantaneous_ops_per_sec`, `used_memory`, `keyspace_misses`\n- Set up alerts for node failures and high latency\n\nThis setup handles node failures gracefully while maintaining write performance."}
{"prompt": "What are the key considerations when implementing a distributed rate limiter?", "response": "Implementing a distributed rate limiter requires careful consideration of several factors:\n\n**Storage Backend:**\n- **Redis**: Fast, supports atomic operations (INCR, EXPIRE), good for sliding window\n- **Database**: More durable but slower, suitable for longer-term limits\n- **In-memory + sync**: Fastest but requires coordination\n\n**Algorithm Choice:**\n1. **Token Bucket**: Allows bursts, smooth rate limiting\n2. **Fixed Window**: Simple but has boundary issues\n3. **Sliding Window**: More accurate but memory-intensive\n4. **Sliding Window Log**: Most accurate, highest memory usage\n\n**Key Design Decisions:**\n- **Partitioning**: By user ID, IP, API key - affects accuracy vs performance\n- **Consistency**: Strong consistency (slower) vs eventual consistency (faster)\n- **Fallback**: Fail open (allow requests) vs fail closed (deny requests)\n\n**Implementation Considerations:**\n```python\n# Example Redis Lua script for atomic operations\nlocal key = KEYS[1]\nlocal limit = tonumber(ARGV[1])\nlocal window = tonumber(ARGV[2])\nlocal current = redis.call('INCR', key)\nif current == 1 then\n    redis.call('EXPIRE', key, window)\nend\nreturn {current, limit}\n```\n\n**Performance:**\n- Cache frequently accessed limits locally\n- Use batching for high-traffic scenarios\n- Consider async updates for better latency\n\n**Monitoring:**\n- Track hit rates, false positives/negatives\n- Monitor backend latency and availability\n- Set up alerts for rate limiter failures"}
{"prompt": "How would you debug a memory leak in a Go application?", "response": "Debugging Go memory leaks requires systematic analysis using Go's built-in tools:\n\n**1. Enable pprof in your application:**\n```go\nimport _ \"net/http/pprof\"\ngo func() {\n    log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n}()\n```\n\n**2. Collect heap profiles:**\n```bash\n# Take snapshots over time\ngo tool pprof http://localhost:6060/debug/pprof/heap\ngo tool pprof -base heap1.pb.gz heap2.pb.gz  # Compare profiles\n```\n\n**3. Analyze with pprof commands:**\n```\n(pprof) top10          # Top memory consumers\n(pprof) list funcname  # Source code with annotations\n(pprof) web           # Visual graph\n(pprof) traces        # Allocation traces\n```\n\n**4. Common leak patterns to look for:**\n- **Goroutine leaks**: `go tool pprof /debug/pprof/goroutine`\n- **Unclosed resources**: files, network connections, channels\n- **Growing slices/maps**: Check for unbounded growth\n- **Event listeners**: Not properly removed\n- **Caches**: Without TTL or size limits\n\n**5. Memory debugging flags:**\n```bash\nGOMEMLIMIT=1GiB        # Set soft memory limit\nGOGC=50               # More aggressive GC\nGODEBUG=gctrace=1     # GC trace output\n```\n\n**6. Prevention strategies:**\n- Use `defer` for cleanup\n- Implement context cancellation\n- Set timeouts on network operations\n- Use sync.Pool for object reuse\n- Regular profile reviews in CI\n\nAlways reproduce the leak in a controlled environment and take multiple heap dumps to identify growth patterns."}
{"prompt": "Explain the CAP theorem and its practical implications for database design.", "response": "The CAP theorem states that in a distributed system, you can only guarantee 2 out of 3 properties:\n\n**Consistency (C)**: All nodes see the same data simultaneously\n**Availability (A)**: System remains operational even during failures\n**Partition tolerance (P)**: System continues despite network partitions\n\n**Practical Implications:**\n\nSince network partitions are inevitable in distributed systems, you must choose between CP or AP:\n\n**CP Systems (Consistency + Partition tolerance):**\n- Examples: MongoDB (default), Redis Cluster, Consul\n- Behavior: Become unavailable during partitions to maintain consistency\n- Use cases: Financial transactions, inventory management\n- Trade-off: May reject writes/reads during network issues\n\n**AP Systems (Availability + Partition tolerance):**\n- Examples: Cassandra, DynamoDB, CouchDB\n- Behavior: Remain available but may serve stale data\n- Use cases: Social media feeds, analytics, caching\n- Trade-off: Eventual consistency, potential data conflicts\n\n**Real-world Considerations:**\n\n1. **Tunable Consistency**: Many systems offer configurable consistency levels\n   - Cassandra: ONE, QUORUM, ALL\n   - MongoDB: Read/write concerns\n\n2. **PACELC Extension**: In the absence of partitions, trade-off between Latency and Consistency\n\n3. **Design Patterns:**\n   - **CQRS**: Separate read/write models\n   - **Event sourcing**: Append-only event logs\n   - **Saga pattern**: Distributed transactions via compensating actions\n\n**Choosing Your Trade-off:**\n- Financial systems: CP (strong consistency required)\n- Social platforms: AP (availability more important than perfect consistency)\n- E-commerce: Hybrid approach (inventory=CP, recommendations=AP)\n\nThe key is understanding your specific consistency requirements and choosing the appropriate system and configuration."}
{"prompt": "What are the best practices for implementing circuit breakers in microservices?", "response": "Circuit breakers prevent cascading failures by monitoring service health and failing fast when dependencies are unhealthy. Here are key implementation practices:\n\n**Core States:**\n1. **CLOSED**: Normal operation, requests pass through\n2. **OPEN**: Requests fail immediately, service is considered down\n3. **HALF-OPEN**: Testing if service has recovered\n\n**Configuration Parameters:**\n```yaml\ncircuit_breaker:\n  failure_threshold: 5        # Failures to trigger OPEN\n  success_threshold: 2        # Successes to close from HALF-OPEN\n  timeout: 60s               # Time before trying HALF-OPEN\n  monitoring_window: 10s     # Window for failure rate calculation\n  failure_rate_threshold: 50  # Percentage threshold\n```\n\n**Implementation Best Practices:**\n\n1. **Granular Circuit Breakers:**\n   - Per-service, per-endpoint, or per-operation\n   - Different thresholds for critical vs non-critical operations\n\n2. **Failure Detection:**\n```go\n// Example criteria for failures\ntype FailureDetector interface {\n    IsFailure(err error, response *http.Response) bool\n}\n\n// Consider failures: timeouts, 5xx errors, connection errors\n// NOT failures: 4xx client errors, business logic errors\n```\n\n3. **Fallback Strategies:**\n   - **Cache**: Return stale data\n   - **Default values**: Safe defaults or empty responses\n   - **Alternative service**: Secondary data source\n   - **Degraded functionality**: Partial feature set\n\n4. **Monitoring and Alerting:**\n```yaml\nmetrics:\n  - circuit_breaker_state (gauge)\n  - circuit_breaker_calls_total (counter)\n  - circuit_breaker_failures_total (counter)\n  - circuit_breaker_fallback_calls_total (counter)\n```\n\n5. **Testing:**\n   - Chaos engineering to test circuit breaker behavior\n   - Load testing with dependency failures\n   - Verify fallback mechanisms work correctly\n\n6. **Bulkhead Pattern Integration:**\n   - Separate thread pools for different dependencies\n   - Isolate failures to prevent resource exhaustion\n\n**Libraries:**\n- **Go**: sony/gobreaker, rubyist/circuitbreaker\n- **Java**: Hystrix, Resilience4j\n- **Python**: pybreaker, circuit-breaker\n\n**Avoid Common Pitfalls:**\n- Don't make circuit breaker logic too complex\n- Ensure fallbacks are also resilient\n- Monitor and tune thresholds based on real traffic patterns\n- Consider the impact of circuit breakers on user experience"}
